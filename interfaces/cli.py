from contextlib import contextmanager
import torch, logging, sys, json, signal
from typing import List, Optional
from pathlib import Path
from llm.model import KINTLanguageModel
from llm.tokenizer import RussianBPETokenizer

# –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
CONFIG_FILE = Path("config.json")

def load_config() -> dict:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ JSON‚Äë—Ñ–∞–π–ª–∞."""
    try:
        with open(CONFIG_FILE, "r", encoding="utf-8") as f:
            config = json.load(f)
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç—Ä–æ–∫–∏ –≤ Path, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if "MODEL_PATH" in config:
            config["MODEL_PATH"] = Path(config["MODEL_PATH"])

        # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è TOP_P
        if "TOP_P" not in config:
            config["TOP_P"] = None  # –∏–ª–∏ –¥—Ä—É–≥–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        return config
    except FileNotFoundError:
        print(f"‚ö†Ô∏è –§–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ {CONFIG_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è.")
        return {
            "MAX_INPUT_LENGTH": 200,
            "MAX_CONTEXT_LENGTH": 1024,
            "MAX_NEW_TOKENS": 150,
            "DEFAULT_TEMPERATURE": 0.8,
            "DEFAULT_TOP_K": 50,
            "MODEL_PATH": Path("epochs/best_model.pth"),
            "TIMEOUT_SECONDS": 60,
            "TOP_P": None,
            "REPETITION_PENALTY": 1.2
        }
    except json.JSONDecodeError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è JSON –∏–∑ {CONFIG_FILE}: {e}")
        sys.exit(1)

CONFIG = load_config()




# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
    handlers=[
        logging.FileHandler("kint_cli.log"),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)



# === –û–ë–†–ê–ë–û–¢–ö–ê –¢–ê–ô–ú–ê–£–¢–ê ===
@contextmanager
def timeout(seconds: int):
    def signal_handler(signum, frame):
        raise TimeoutError(f"–ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ ({seconds} —Å–µ–∫)")
    
    signal.signal(signal.SIGALRM, signal_handler)
    signal.alarm(seconds)
    try:
        yield
    finally:
        signal.alarm(0)



# === –°–ï–†–í–ò–°–´ ===
class TokenizerService:
    def __init__(self, tokenizer: RussianBPETokenizer):
        self.tokenizer = tokenizer

    def encode(self, text: str) -> List[int]:
        if not text.strip():
            raise ValueError("–ü—É—Å—Ç–æ–π –≤–≤–æ–¥")
        return self.tokenizer.encode(text)

    def decode(self, tokens: List[int]) -> str:
        return self.tokenizer.decode(tokens)



class GenerationService:
    def __init__(
        self,
        model: KINTLanguageModel,
        tokenizer_service: TokenizerService,
        device: str
    ):
        self.model = model.to(device).eval()
        self.tokenizer_service = tokenizer_service
        self.device = device

    def generate(
        self,
        prompt: str,
        max_new_tokens: int = CONFIG["MAX_NEW_TOKENS"],
        temperature: float = CONFIG["DEFAULT_TEMPERATURE"],
        top_k: int = CONFIG["DEFAULT_TOP_K"],
        top_p: Optional[float] = CONFIG["TOP_P"],
        repetition_penalty: float = CONFIG["REPETITION_PENALTY"]
    ) -> str:
        try:
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            tokens = self.tokenizer_service.encode(prompt)
            if len(tokens) > CONFIG["MAX_INPUT_LENGTH"]:
                raise ValueError(
                    f"–ü—Ä–µ–≤—ã—à–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –≤–≤–æ–¥–∞ ({len(tokens)} > {CONFIG['MAX_INPUT_LENGTH']})"
                )

            input_tokens = torch.tensor([tokens], dtype=torch.long).to(self.device)

            with torch.no_grad(), timeout(CONFIG["TIMEOUT_SECONDS"]):
                for _ in range(max_new_tokens):
                    logits = self.model(input_tokens)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ logits —è–≤–ª—è–µ—Ç—Å—è —Ç–µ–Ω–∑–æ—Ä–æ–º
                    if isinstance(logits, list):
                        logits = torch.tensor(logits, device=self.device)
                    
                    logits = logits[:, -1, :] / temperature

                    # –¢–æ–ø‚Äëk —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
                    if top_k and top_k > 0:
                        v, _ = torch.topk(logits, top_k)
                        logits[logits < v[:, [-1]]] = -float('inf')

                    # Nucleus sampling (top‚Äëp)
                    if top_p and top_p > 0:
                        sorted_logits, sorted_indices = torch.sort(logits, descending=True)
                        cumulative_probs = torch.cumsum(
                            torch.softmax(sorted_logits, dim=-1), dim=-1
                        )
                        filtered_indices = sorted_indices[:, cumulative_probs > top_p]
                        logits[:, filtered_indices] = -float('inf')

                    # –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä—ã
                    for token in set(input_tokens[0].tolist()):
                        logits[:, token] /= repetition_penalty

                    probs = torch.softmax(logits, dim=-1)
                    next_token = torch.multinomial(probs, num_samples=1)

                    input_tokens = torch.cat([input_tokens, next_token], dim=1)

                    if (next_token.item() == self.tokenizer_service.tokenizer.eos_id
                        or input_tokens.size(1) >= CONFIG["MAX_CONTEXT_LENGTH"]):
                        break

            generated_tokens = input_tokens[0].tolist()
            return self.tokenizer_service.decode(generated_tokens)

        except TimeoutError as e:
            logger.error(f"–¢–∞–π–º–∞—É—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            return "‚ùå –ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                logger.error("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤–∏–¥–µ–æ–ø–∞–º—è—Ç–∏")
                return "‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤–∏–¥–µ–æ–ø–∞–º—è—Ç–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –¥–ª–∏–Ω—É –≤–≤–æ–¥–∞."
            else:
                logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
                return f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}"
        except Exception as e:
            logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
            return f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞: {e}"

# === CLI ===
class KINTCLI:
    """KINT CLI —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏"""
    def __init__(self):
        if hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            self.device = "mps"  # Metal Performance Shaders
        else:
            self.device = "cpu"
        self.history: List[str] = []
        self.temperature = 0.6
        self.top_k = 100
        self.top_p = 0.98
        self.repetition_penalty = 1.1
        self.enable_reasoning = True
        self.enable_future_prediction = True
        self.reasoning_depth = 50
        
        logger.info("üß† KINT –ú–ï–ì–ê–ò–ù–¢–ï–õ–õ–ï–ö–¢ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")

    def load_model(self) -> Optional[GenerationService]:
        try:
            tokenizer = RussianBPETokenizer()
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –ú–ï–ì–ê–ú–û–î–ï–õ–ò
            model = KINTLanguageModel(
                vocab_size=tokenizer.vocab_size,
                dim=2048,
                depth=64,
                heads=64,
                quantum_qubits=32,
                num_reasoning_steps=50
            )

            if CONFIG["MODEL_PATH"].exists():
                state_dict = torch.load(
                    CONFIG["MODEL_PATH"],
                    map_location=self.device,
                    weights_only=True
                )
                model.load_state_dict(state_dict, strict=False)
                logger.info(f"‚úÖ –ú–ï–ì–ê–ú–û–î–ï–õ–¨ –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {CONFIG['MODEL_PATH']}")
            else:
                logger.warning("‚ö†Ô∏è  –ú–ï–ì–ê–ú–û–î–ï–õ–¨ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è.")

            model.eval()
            if self.device == "cuda":
                model = model.half()

            tokenizer_service = TokenizerService(tokenizer)
            return GenerationService(model, tokenizer_service, self.device)

        except Exception as e:
            logger.critical(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ú–ï–ì–ê–ú–û–î–ï–õ–ò: {e}")
            return None

    def show_help(self):
        print(
            "üöÄ KINT –ú–ï–ì–ê–ò–ù–¢–ï–õ–õ–ï–ö–¢ - –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:\n"
            "- exit/quit: –≤—ã—Ö–æ–¥\n"
            "- help: —Å–ø—Ä–∞–≤–∫–∞\n"
            "- reason <–≥–ª—É–±–∏–Ω–∞>: —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≥–ª—É–±–∏–Ω—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (1-50)\n"
            "- temp <–∑–Ω–∞—á–µ–Ω–∏–µ>: —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ (0.1-1.5)\n"
            "- prediction on/off: –±—É–¥—É—â–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n"
            "- analyze <—Ç–µ–∫—Å—Ç>: –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞\n"
            "- predict <–∫–æ–Ω—Ç–µ–∫—Å—Ç>: –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å —Ä–∞–∑–≤–∏—Ç–∏–µ\n"
            "- reset: —Å–±—Ä–æ—Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"
        )

    def parse_command(self, user_input: str) -> bool:
        cmd = user_input.lower().strip()

        if cmd in {"exit", "quit"}:
            print("üåü –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            return False

        elif cmd == "help":
            self.show_help()
            return True

        elif cmd.startswith("reason "):
            try:
                value = int(cmd.split()[1])
                if 1 <= value <= 50:
                    self.reasoning_depth = value
                    print(f"‚úÖ –ì–ª—É–±–∏–Ω–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π: {self.reasoning_depth}")
                else:
                    print("‚ùå –ó–Ω–∞—á–µ–Ω–∏–µ –æ—Ç 1 –¥–æ 50")
            except:
                print("‚ùå –ü—Ä–∏–º–µ—Ä: reason 30")
            return True

        elif cmd.startswith("prediction "):
            state = cmd.split()[1].lower()
            self.enable_future_prediction = state == "on"
            print(f"‚úÖ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {'–í–ö–õ–û' if self.enable_future_prediction else '–í–´–ö–õ–û'}")
            return True

        elif cmd.startswith("analyze "):
            text = user_input[8:]
            print("üîç –ê–Ω–∞–ª–∏–∑...\n")
            # –ó–¥–µ—Å—å –±—É–¥–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            print(f"üìä –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞: '{text[:50]}...'")
            return True

        return False

    def _get_device_info(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞."""
        if self.device == "cuda":
            # NVIDIA GPU
            gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else "NVIDIA GPU"
            return f"‚úÖ –ú–æ–¥–µ–ª—å –Ω–∞ GPU (CUDA): {gpu_name}"
        elif self.device == "mps":
            # Apple Silicon (M1/M2/M3)
            return "‚úÖ –ú–æ–¥–µ–ª—å –Ω–∞ GPU (Apple MPS: Apple Silicon)"
        elif self.device == "cpu":
            # CPU (—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç)
            import platform
            cpu_info = platform.processor() or platform.machine()
            return f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–∞ CPU ({cpu_info}): –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–¥–ª–µ–Ω–Ω–æ–π"
        elif self.device.startswith("xla"):
            # TPU (Google Cloud)
            return "‚úÖ –ú–æ–¥–µ–ª—å –Ω–∞ TPU (Google XLA)"
        elif self.device.startswith("hpu"):
            # Habana Goya (Intel)
            return "‚úÖ –ú–æ–¥–µ–ª—å –Ω–∞ GPU (Habana Goya)"
        elif self.device.startswith("ort"):
            # ONNX Runtime
            return "‚úÖ –ú–æ–¥–µ–ª—å –Ω–∞ —É—Å–∫–æ—Ä–∏—Ç–µ–ª–µ (ONNX Runtime)"
        elif self.device.startswith("npu"):
            # Huawei Ascend
            return "‚úÖ –ú–æ–¥–µ–ª—å –Ω–∞ NPU (Huawei Ascend)"
        else:
            # –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π/–Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –±—ç–∫–µ–Ω–¥
            return f"‚ÑπÔ∏è –ú–æ–¥–µ–ª—å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: {self.device} (–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø)"


    def run(self):
        print("üöÄ KINT –ú–ï–ì–ê–ò–ù–¢–ï–õ–õ–ï–ö–¢ - –°–£–ü–ï–† –ò–ò")
        print("=" * 60)
        print("–í–≤–µ–¥–∏—Ç–µ 'help' –¥–ª—è —Å–ø—Ä–∞–≤–∫–∏")
        print("=" * 60)

        generation_service = self.load_model()
        if not generation_service:
            sys.exit(1)

        print(self._get_device_info())

        while True:
            try:
                user_input = input("\nüß† –í—ã> ").strip()
                if not user_input:
                    continue

                if self.parse_command(user_input):
                    continue

                print("‚ö° KINT> ", end="", flush=True)
                response = generation_service.generate(
                    prompt=user_input,
                    temperature=self.temperature,
                    top_k=self.top_k,
                    top_p=self.top_p,
                    repetition_penalty=self.repetition_penalty
                )
                print(response)

                self.history.append(f"–í—ã> {user_input}")
                self.history.append(f"KINT> {response}")

            except KeyboardInterrupt:
                print("\nüåü –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                break
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞: {e}")
                print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

def run_cli():
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å CLI"""
    cli = KINTCLI()
    cli.run()
