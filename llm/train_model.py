# ================================================================
# train.py –¥–ª—è KINT (LLM —Å –ª–æ–∫–∞–ª—å–Ω–æ–π –∫–≤–∞–Ω—Ç–æ–≤–æ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–π)
# ================================================================

import torch, time, json, tempfile, logging
import torch.nn.functional as F
from pathlib import Path
from torch.utils.data import Dataset, DataLoader
from torch.optim.lr_scheduler import OneCycleLR, CosineAnnealingWarmRestarts
from llm.model import KINTLanguageModel
from llm.tokenizer import RussianBPETokenizer
from datetime import datetime
from typing import Optional, Dict, Any, Tuple
import numpy as np
from collections import defaultdict

# ================================================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ò –ü–ê–†–ê–ú–ï–¢–†–´
# ================================================================

MAX_EPOCHS = 50
BATCH_SIZE = 8
BLOCK_SIZE = 256
LEARNING_RATE = 5e-5
WEIGHT_DECAY = 1e-5
PATIENCE = 5
GRAD_CLIP = 1.0

LOSS_WEIGHTS = {
    "lm": 1.0,
    "quantum": 0.1,
    "contrastive": 0.05
}

EPOCH_STATE_FILE = Path("epochs/epoch_state.json")
BEST_MODEL_PATH = Path("epochs/best_model.pth")
LOG_DIR = Path("epochs/logs")
LOG_FILE = LOG_DIR / f"train_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"

EPOCH_DIR = Path("epochs/saved_epochs")
EPOCH_DIR.mkdir(parents=True, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
    handlers=[
        logging.FileHandler(LOG_FILE, encoding="utf-8"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ================================================================
# DATASET –° –£–õ–£–ß–®–ï–ù–ù–û–ô –ê–£–ì–ú–ï–ù–¢–ê–¶–ò–ï–ô
# ================================================================

class TextDataset(Dataset):
    """Dataset —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è"""
    def __init__(self, file_path: str, tokenizer: RussianBPETokenizer, block_size: int, 
                 augment: bool = True, cache_size: int = 10000):
        self.tokenizer = tokenizer
        self.block_size = block_size
        self.augment = augment
        self.cache_size = cache_size

        with open(file_path, "r", encoding="utf-8") as f:
            text = f.read()

        tokens = tokenizer.encode(text, out_type="int")
        self.examples = []
        self.token_counts = defaultdict(int)
        
        for i in range(0, len(tokens) - block_size, block_size):
            block = tokens[i:i + block_size]
            example = torch.tensor(block, dtype=torch.long)
            self.examples.append(example)
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤
            for token in block:
                self.token_counts[token] += 1

        if augment:
            self._augment_data()

        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.examples)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        logger.info(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã: {len(self.token_counts)}")

    def _augment_data(self):
        """–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –≤–∞—Ä–∏–∞–Ω—Ç—ã –±–ª–æ–∫–æ–≤"""
        original_len = len(self.examples)
        for i in range(min(original_len, self.cache_size)):
            example = self.examples[i]
            # –ü–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫–∞
            shuffled = example[torch.randperm(len(example))]
            self.examples.append(shuffled)
            # –û–±—Ä–∞—Ç–Ω–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
            reversed_ex = example.flip(0)
            self.examples.append(reversed_ex)

    def __len__(self) -> int:
        return len(self.examples)

    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        example = self.examples[idx]
        input_ids = example[:-1]
        labels = example[1:]
        return {"input_ids": input_ids, "labels": labels}

# ================================================================
# –ú–ï–¢–†–ò–ö–ò –ò –ú–û–ù–ò–¢–û–†–ò–ù–ì
# ================================================================

class TrainingMetrics:
    """–û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –æ–±—É—á–µ–Ω–∏—è"""
    def __init__(self):
        self.history = defaultdict(list)
        self.best_metrics = {}
        
    def update(self, metrics: Dict[str, float], epoch: int):
        """–û–±–Ω–æ–≤–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏"""
        for key, value in metrics.items():
            self.history[key].append((epoch, value))
            
        # –û–±–Ω–æ–≤–∏—Ç—å –ª—É—á—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
        for key, value in metrics.items():
            if key not in self.best_metrics or value < self.best_metrics[key]:
                self.best_metrics[key] = value
    
    def get_summary(self) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–≤–æ–¥–∫—É –ª—É—á—à–∏—Ö –º–µ—Ç—Ä–∏–∫"""
        lines = ["=== –õ–£–ß–®–ò–ï –ú–ï–¢–†–ò–ö–ò ==="]
        for key, value in self.best_metrics.items():
            lines.append(f"{key}: {value:.6f}")
        return "\n".join(lines)

class ProgressTracker:
    """–û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
    def __init__(self, total_epochs: int):
        self.total_epochs = total_epochs
        self.start_time = time.time()
        self.epoch_times = []
    
    def log_epoch(self, epoch: int, loss: float, lr: float):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–ø–æ—Ö–∏"""
        elapsed = time.time() - self.start_time
        self.epoch_times.append(elapsed)
        
        avg_time = np.mean(self.epoch_times[-10:]) if len(self.epoch_times) > 0 else 0
        remaining = avg_time * (self.total_epochs - epoch - 1)
        
        hours, remainder = divmod(remaining, 3600)
        minutes, _ = divmod(remainder, 60)
        
        logger.info(
            f"Epoch {epoch+1}/{self.total_epochs} | Loss: {loss:.6f} | "
            f"LR: {lr:.2e} | ETA: {int(hours)}h {int(minutes)}m"
        )

# ================================================================
# –°–û–•–†–ê–ù–ï–ù–ò–ï –ò –í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ò–ï
# ================================================================

def save_checkpoint(
    model: torch.nn.Module,
    optimizer: torch.optim.Optimizer,
    epoch: int,
    best_loss: float,
    patience_counter: int,
    metrics: TrainingMetrics,
    checkpoint_path: Path,
    scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None
):
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—É—é —Ç–æ—á–∫—É"""
    checkpoint = {
        "epoch": epoch,
        "model_state": model.state_dict(),
        "optimizer_state": optimizer.state_dict(),
        "best_loss": best_loss,
        "patience_counter": patience_counter,
        "metrics": dict(metrics.history),
        "timestamp": datetime.now().isoformat(),
        "learning_rate": optimizer.param_groups[0]["lr"]
    }
    
    if scheduler:
        checkpoint["scheduler_state"] = scheduler.state_dict()
    
    try:
        with tempfile.NamedTemporaryFile("wb", delete=False) as tmp:
            torch.save(checkpoint, tmp.name)
            Path(tmp.name).replace(checkpoint_path)
        logger.info(f"‚úÖ –ö–æ–Ω—Ç—Ä–æ–ª—å–Ω–∞—è —Ç–æ—á–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {checkpoint_path}")
        return True
    except Exception as e:
        logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—É—é —Ç–æ—á–∫—É: {e}")
        return False

def load_checkpoint(
    model: torch.nn.Module,
    optimizer: torch.optim.Optimizer,
    checkpoint_path: Path,
    device: torch.device,
    scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None
) -> Tuple[int, float, int, TrainingMetrics]:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—É—é —Ç–æ—á–∫—É"""
    if not checkpoint_path.exists():
        logger.info("–ö–æ–Ω—Ç—Ä–æ–ª—å–Ω–∞—è —Ç–æ—á–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ù–∞—á–∏–Ω–∞–µ–º —Å –Ω—É–ª—è.")
        return 0, float("inf"), 0, TrainingMetrics()
    
    try:
        checkpoint = torch.load(checkpoint_path, map_location=device)
        
        model.load_state_dict(checkpoint["model_state"], strict=False)
        optimizer.load_state_dict(checkpoint["optimizer_state"])
        
        if scheduler and "scheduler_state" in checkpoint:
            scheduler.load_state_dict(checkpoint["scheduler_state"])
        
        metrics = TrainingMetrics()
        if "metrics" in checkpoint:
            metrics.history = defaultdict(list, checkpoint["metrics"])
        
        logger.info(f"‚úÖ –ö–æ–Ω—Ç—Ä–æ–ª—å–Ω–∞—è —Ç–æ—á–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ —ç–ø–æ—Ö–∏ {checkpoint['epoch'] + 1}")
        return (
            checkpoint["epoch"] + 1,
            checkpoint.get("best_loss", float("inf")),
            checkpoint.get("patience_counter", 0),
            metrics
        )
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–æ–π —Ç–æ—á–∫–∏: {e}")
        return 0, float("inf"), 0, TrainingMetrics()

# ================================================================
# –§–£–ù–ö–¶–ò–ò –û–ë–£–ß–ï–ù–ò–Ø
# ================================================================

def compute_loss(
    logits: torch.Tensor,
    labels: torch.Tensor,
    model: torch.nn.Module,
    hidden_states: Optional[torch.Tensor] = None,
    augmented_states: Optional[torch.Tensor] = None
) -> Tuple[torch.Tensor, Dict[str, float]]:
    """–í—ã—á–∏—Å–ª–∏—Ç—å –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—É—é –ø–æ—Ç–µ—Ä—é"""
    # –û—Å–Ω–æ–≤–Ω–∞—è –ø–æ—Ç–µ—Ä—è —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏
    lm_loss = F.cross_entropy(
        logits.view(-1, logits.size(-1)),
        labels.view(-1)
    )
    
    losses = {"lm": lm_loss.item()}
    total_loss = LOSS_WEIGHTS["lm"] * lm_loss
    
    # –ö–≤–∞–Ω—Ç–æ–≤–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
    if hasattr(model, 'quantum_layer'):
        quantum_weights = model.quantum_layer.weights
        quantum_entanglers = model.quantum_layer.entanglers
        quantum_loss = (
            torch.norm(quantum_weights) ** 2 +
            torch.norm(quantum_entanglers) ** 2
        ) / (quantum_weights.numel() + quantum_entanglers.numel())
        
        losses["quantum"] = quantum_loss.item()
        total_loss = total_loss + LOSS_WEIGHTS["quantum"] * quantum_loss
    
    # –ö–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–∞—è –ø–æ—Ç–µ—Ä—è (–µ—Å–ª–∏ –µ—Å—Ç—å —Å–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è)
    if hidden_states is not None and augmented_states is not None:
        try:
            contrastive_loss = model.compute_contrastive_loss(hidden_states, augmented_states)
            losses["contrastive"] = contrastive_loss.item()
            total_loss = total_loss + LOSS_WEIGHTS["contrastive"] * contrastive_loss
        except Exception as e:
            logger.debug(f"–ö–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–∞—è –ø–æ—Ç–µ—Ä—è –ø—Ä–æ–ø—É—â–µ–Ω–∞: {e}")
    
    return total_loss, losses

def train_epoch(
    model: torch.nn.Module,
    dataloader: DataLoader,
    optimizer: torch.optim.Optimizer,
    device: torch.device,
    epoch: int
) -> Dict[str, float]:
    """–û–±—É—á–∏—Ç—å –æ–¥–Ω—É —ç–ø–æ—Ö—É"""
    model.train()
    epoch_loss = 0.0
    loss_components = defaultdict(float)
    
    for batch_idx, batch in enumerate(dataloader):
        input_ids = batch["input_ids"].to(device)
        labels = batch["labels"].to(device)
        
        optimizer.zero_grad()
        
        # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
        output = model(input_ids, return_logits=True, enable_reasoning=True)
        
        if isinstance(output, dict):
            logits = output['logits']
        else:
            logits = output
        
        # –í—ã—á–∏—Å–ª–∏—Ç—å –ø–æ—Ç–µ—Ä—é
        loss, loss_dict = compute_loss(logits, labels, model)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN/Inf
        if torch.isnan(loss) or torch.isinf(loss):
            logger.warning(f"Loss —Å–æ–¥–µ—Ä–∂–∏—Ç NaN/Inf –Ω–∞ –±–∞—Ç—á–µ {batch_idx}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            continue
        
        # –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥
        loss.backward()
        
        # –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –∫–ª–∏–ø–ø–∏–Ω–≥
        if GRAD_CLIP > 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)
        
        optimizer.step()
        
        # –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        epoch_loss += loss.item()
        for key, value in loss_dict.items():
            loss_components[key] += value
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞—Ç—á–∞ (–∫–∞–∂–¥—ã–µ 50)
        if batch_idx % 50 == 0:
            avg_loss = epoch_loss / (batch_idx + 1)
            logger.debug(f"  Batch {batch_idx} | Loss: {avg_loss:.6f}")
    
    # –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –∑–∞ —ç–ø–æ—Ö—É
    avg_loss = epoch_loss / len(dataloader)
    metrics = {"loss": avg_loss}
    
    for key, value in loss_components.items():
        metrics[f"{key}_loss"] = value / len(dataloader)
    
    return metrics

@torch.no_grad()
def evaluate(
    model: torch.nn.Module,
    dataloader: DataLoader,
    device: torch.device
) -> Dict[str, float]:
    """–û—Ü–µ–Ω–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ"""
    model.eval()
    total_loss = 0.0
    
    for batch in dataloader:
        input_ids = batch["input_ids"].to(device)
        labels = batch["labels"].to(device)
        
        output = model(input_ids, return_logits=True)
        logits = output['logits'] if isinstance(output, dict) else output
        
        loss, _ = compute_loss(logits, labels, model)
        
        if not (torch.isnan(loss) or torch.isinf(loss)):
            total_loss += loss.item()
    
    return {"eval_loss": total_loss / len(dataloader)}

# ================================================================
# –û–°–ù–û–í–ù–û–ô –¶–ò–ö–õ –û–ë–£–ß–ï–ù–ò–Ø
# ================================================================

def train():
    """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è"""
    global MAX_EPOCHS
    
    logger.info("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ú–ï–ì–ê–û–ë–£–ß–ï–ù–ò–ï KINT...")
    
    # === –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø ===
    tokenizer = RussianBPETokenizer()
    model = KINTLanguageModel(
        vocab_size=tokenizer.vocab_size,
        dim=2048,
        depth=64,
        heads=64,
        quantum_qubits=32,
        num_reasoning_steps=50
    )
    
    # –í—ã–±–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        logger.info("‚úÖ GPU (Apple Metal) - MPS")
    elif torch.cuda.is_available():
        device = torch.device("cuda")
        logger.info("‚úÖ GPU (NVIDIA) - CUDA")
    else:
        device = torch.device("cpu")
        logger.warning("‚ö†Ô∏è CPU (–±—É–¥–µ—Ç –º–µ–¥–ª–µ–Ω–Ω–æ)")
    
    model.to(device)
    
    # === –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• ===
    try:
        dataset = TextDataset("data/corpus.txt", tokenizer, BLOCK_SIZE, augment=True)
    except FileNotFoundError:
        logger.error("‚ùå –§–∞–π–ª data/corpus.txt –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return
    
    train_size = int(0.9 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(
        dataset, 
        [train_size, val_size]
    )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        pin_memory=(device.type == "cuda"),
        num_workers=0
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=BATCH_SIZE,
        pin_memory=(device.type == "cuda"),
        num_workers=0
    )
    
    logger.info(f"üìä –î–∞–Ω–Ω—ã–µ: {len(train_dataset)} –æ–±—É—á–µ–Ω–∏–µ, {len(val_dataset)} –≤–∞–ª–∏–¥–∞—Ü–∏—è")
    
    # === –û–ü–¢–ò–ú–ò–ó–ê–¢–û–† –ò SCHEDULER ===
    optimizer = torch.optim.Adam(
        model.parameters(),
        lr=LEARNING_RATE,
        weight_decay=WEIGHT_DECAY,
        betas=(0.9, 0.98)
    )
    
    scheduler = OneCycleLR(
        optimizer,
        max_lr=LEARNING_RATE,
        total_steps=MAX_EPOCHS * len(train_loader),
        pct_start=0.1,
        anneal_strategy='cos'
    )
    
    # === –í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ò–ï –°–û–°–¢–û–Ø–ù–ò–Ø ===
    LOG_DIR.mkdir(parents=True, exist_ok=True)
    EPOCH_DIR.mkdir(parents=True, exist_ok=True)
    
    start_epoch, best_loss, patience_counter, metrics_tracker = load_checkpoint(
        model, optimizer, EPOCH_STATE_FILE, device, scheduler
    )
    
    progress_tracker = ProgressTracker(MAX_EPOCHS)
    training_metrics = TrainingMetrics()
    
    # === –û–°–ù–û–í–ù–û–ô –¶–ò–ö–õ ===
    for epoch in range(start_epoch, MAX_EPOCHS):
        try:
            # –û–±—É—á–µ–Ω–∏–µ
            train_metrics = train_epoch(model, train_loader, optimizer, device, epoch)
            scheduler.step()
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è (–∫–∞–∂–¥—ã–µ 5 —ç–ø–æ—Ö)
            if epoch % 5 == 0:
                val_metrics = evaluate(model, val_loader, device)
                train_metrics.update(val_metrics)
            
            # –û–±–Ω–æ–≤–∏—Ç—å –º–µ —Ç—Ä–∏–∫–∏
            training_metrics.update(train_metrics, epoch)
            
            current_loss = train_metrics["loss"]
            current_lr = optimizer.param_groups[0]["lr"]
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
            progress_tracker.log_epoch(epoch, current_loss, current_lr)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–ª—É—á—à–µ–Ω–∏—è
            if current_loss < best_loss:
                best_loss = current_loss
                patience_counter = 0
                
                # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
                save_checkpoint(
                    model, optimizer, epoch, best_loss, 
                    patience_counter, training_metrics, BEST_MODEL_PATH, scheduler
                )
                logger.info(f"üéØ –ù–æ–≤—ã–π –ª—É—á—à–∏–π loss: {best_loss:.6f}")
            else:
                patience_counter += 1
                
                if patience_counter >= PATIENCE:
                    logger.info(f"‚èπÔ∏è –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}")
                    break
            
            # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            if epoch % 10 == 0:
                epoch_checkpoint = EPOCH_DIR / f"model_epoch_{epoch+1}.pth"
                save_checkpoint(
                    model, optimizer, epoch, best_loss,
                    patience_counter, training_metrics, epoch_checkpoint, scheduler
                )
        
        except KeyboardInterrupt:
            logger.info("‚è∏Ô∏è –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            break
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}: {e}")
            continue
    
    # === –§–ò–ù–ê–õ ===
    logger.info("\n" + "="*60)
    logger.info("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
    logger.info("="*60)
    logger.info(training_metrics.get_summary())
    logger.info(f"–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {BEST_MODEL_PATH}")
    logger.info(f"–õ–æ–≥–∏: {LOG_FILE}")

if __name__ == "__main__":
    train()