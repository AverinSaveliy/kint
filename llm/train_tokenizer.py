import sentencepiece as spm
from pathlib import Path
import re
import json
from typing import Optional, List, Dict, Tuple
import logging
from datetime import datetime
import os

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s"
)
logger = logging.getLogger(__name__)

# –ü—É—Ç–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
DATA_PATH = Path("data/corpus.txt")
MODEL_PREFIX = "tokenizer"
STATS_FILE = "tokenizer_stats.json"

class TextCleaner:
    """–û—á–∏—Å—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–æ–º"""
    
    def __init__(self, aggressive: bool = False):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ—á–∏—Å—Ç–∏—Ç–µ–ª—è —Ç–µ–∫—Å—Ç–∞.
        
        Args:
            aggressive: –ï—Å–ª–∏ True, –ø—Ä–∏–º–µ–Ω—è–µ—Ç –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é –æ—á–∏—Å—Ç–∫—É
        """
        self.aggressive = aggressive
        self.stats = {
            'lines_processed': 0,
            'lines_cleaned': 0,
            'chars_removed': 0,
            'avg_line_length_before': 0,
            'avg_line_length_after': 0
        }
    
    def clean_text(self, line: str) -> str:
        """
        –û—á–∏—â–∞–µ—Ç —Å—Ç—Ä–æ–∫—É –æ—Ç —à—É–º–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç.
        
        Args:
            line: –í—Ö–æ–¥–Ω–∞—è —Å—Ç—Ä–æ–∫–∞
            
        Returns:
            –û—á–∏—â–µ–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞
        """
        original_len = len(line)
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        line = re.sub(r'\s+', ' ', line)
        
        # –£–¥–∞–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        if self.aggressive:
            # –£–¥–∞–ª—è–µ–º –æ–¥–∏–Ω–æ—á–Ω—ã–µ —Ü–∏—Ñ—Ä—ã
            line = re.sub(r'\b\d\b', '', line)
            # –£–¥–∞–ª—è–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ –ø–æ–¥—Å—Ç—Ä–æ–∫–∏
            line = re.sub(r'\b[a-zA-Z]{1,2}\b', '', line)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫–∞–≤—ã—á–∫–∏
        line = line.replace('¬´', '"').replace('¬ª', '"')
        line = line.replace(''', "'").replace(''', "'")
        line = line.replace('‚Äû', '"').replace('"', '"')
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–∏—Ä–µ
        line = line.replace('‚Äì', '-').replace('‚Äî', '-')
        
        # –£–¥–∞–ª—è–µ–º –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã (–∫—Ä–æ–º–µ –ø—Ä–æ–±–µ–ª—å–Ω—ã—Ö)
        line = ''.join(c for c in line if ord(c) >= 32 or c in '\n\t\r')
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ
        line = line.strip()
        
        self.stats['chars_removed'] += original_len - len(line)
        
        return line
    
    def is_valid_line(self, line: str, min_length: int = 5) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –≤–∞–ª–∏–¥–Ω–∞ –ª–∏ —Å—Ç—Ä–æ–∫–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.
        
        Args:
            line: –°—Ç—Ä–æ–∫–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            min_length: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å—Ç—Ä–æ–∫–∏
            
        Returns:
            True –µ—Å–ª–∏ –≤–∞–ª–∏–¥–Ω–∞
        """
        if not line:
            return False
        
        if len(line) < min_length:
            return False
        
        # –ù–µ –º–µ–Ω–µ–µ 50% –±—É–∫–≤ –≤ —Å—Ç—Ä–æ–∫–µ
        letter_count = sum(1 for c in line if c.isalpha())
        if letter_count / len(line) < 0.5:
            return False
        
        # –ù–µ –±–æ–ª–µ–µ 90% –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
        char_counts = {}
        for c in line:
            char_counts[c] = char_counts.get(c, 0) + 1
        max_char_ratio = max(char_counts.values()) / len(line)
        if max_char_ratio > 0.9:
            return False
        
        return True
    
    def get_stats(self) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ—á–∏—Å—Ç–∫–∏"""
        return self.stats.copy()

def estimate_vocab_size(text_size: int) -> int:
    """
    –≠–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —Å–ª–æ–≤–∞—Ä—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–º–µ—Ä–∞ –∫–æ—Ä–ø—É—Å–∞.
    
    Args:
        text_size: –†–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –≤ –±–∞–π—Ç–∞—Ö
        
    Returns:
        –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è
    """
    if text_size < 10_000:
        return 500
    elif text_size < 100_000:
        return 2_000
    elif text_size < 1_000_000:
        return 8_000
    elif text_size < 10_000_000:
        return 16_000
    elif text_size < 100_000_000:
        return 32_000
    else:
        return 50_000

def validate_corpus(file_path: Path) -> Tuple[bool, List[str]]:
    """
    –í–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å –∫–æ—Ä–ø—É—Å –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º.
    
    Args:
        file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ—Ä–ø—É—Å–∞
        
    Returns:
        (is_valid, issues) - –∫–æ—Ä—Ç–µ–∂ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –∏ —Å–ø–∏—Å–∫–∞ –ø—Ä–æ–±–ª–µ–º
    """
    issues = []
    
    if not file_path.exists():
        issues.append(f"–§–∞–π–ª –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {file_path}")
        return False, issues
    
    file_size = file_path.stat().st_size
    if file_size == 0:
        issues.append("–§–∞–π–ª –ø—É—Å—Ç")
        return False, issues
    
    if file_size < 1_000:
        issues.append(f"–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π ({file_size} –±–∞–π—Ç). –ú–∏–Ω–∏–º—É–º 1 KB.")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            first_lines = [f.readline() for _ in range(10)]
        
        if not any(first_lines):
            issues.append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Å—Ç—Ä–æ–∫–∏ –∏–∑ —Ñ–∞–π–ª–∞")
    except UnicodeDecodeError:
        issues.append("–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ UTF-8. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–π—Ç–µ —Ñ–∞–π–ª.")
        return False, issues
    except Exception as e:
        issues.append(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
        return False, issues
    
    return len(issues) == 0, issues

def prepare_corpus(
    input_path: str,
    output_path: str = "corpus_cleaned.txt",
    min_line_length: int = 5,
    aggressive_clean: bool = False
) -> Dict:
    """
    –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –∏ –æ—á–∏—Å—Ç–∏—Ç—å –∫–æ—Ä–ø—É—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞.
    
    Args:
        input_path: –ü—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É
        output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—á–∏—â–µ–Ω–Ω–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞
        min_line_length: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å—Ç—Ä–æ–∫–∏
        aggressive_clean: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é –æ—á–∏—Å—Ç–∫—É
        
    Returns:
        –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    input_file = Path(input_path)
    output_file = Path(output_path)
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è
    is_valid, issues = validate_corpus(input_file)
    if not is_valid:
        logger.error("‚ùå –û—à–∏–±–∫–∏ –∫–æ—Ä–ø—É—Å–∞:")
        for issue in issues:
            logger.error(f"  - {issue}")
        return {'error': issues}
    
    logger.info(f"üìÇ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ—Ä–ø—É—Å–∞: {input_file}")
    logger.info(f"   –†–∞–∑–º–µ—Ä: {input_file.stat().st_size / 1_000_000:.2f} MB")
    
    cleaner = TextCleaner(aggressive=aggressive_clean)
    
    try:
        with open(input_file, 'r', encoding='utf-8', errors='replace') as f_in:
            with open(output_file, 'w', encoding='utf-8') as f_out:
                for line_idx, line in enumerate(f_in):
                    cleaner.stats['lines_processed'] += 1
                    
                    cleaned = cleaner.clean_text(line)
                    
                    if cleaner.is_valid_line(cleaned, min_line_length):
                        f_out.write(cleaned + '\n')
                        cleaner.stats['lines_cleaned'] += 1
                    
                    if (line_idx + 1) % 10000 == 0:
                        logger.debug(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {line_idx + 1} —Å—Ç—Ä–æ–∫...")
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        input_size = input_file.stat().st_size
        output_size = output_file.stat().st_size
        stats = cleaner.get_stats()
        stats['input_size'] = input_size
        stats['output_size'] = output_size
        stats['compression_ratio'] = output_size / input_size if input_size > 0 else 0
        
        logger.info(f"‚úÖ –ö–æ—Ä–ø—É—Å –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω:")
        logger.info(f"   –°—Ç—Ä–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['lines_processed']}")
        logger.info(f"   –°—Ç—Ä–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {stats['lines_cleaned']}")
        logger.info(f"   –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è: {stats['compression_ratio']:.2%}")
        
        return stats
    
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∫–æ—Ä–ø—É—Å–∞: {e}")
        return {'error': str(e)}

def train(
    data_path: str = "data/corpus.txt",
    model_prefix: str = "tokenizer",
    vocab_size: Optional[int] = None,
    character_coverage: float = 0.9995,
    user_defined_symbols: Optional[list] = None,
    model_type: str = "bpe",
    train_params: Optional[Dict] = None
) -> bool:
    """
    –û–±—É—á–∞–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä SentencePiece –Ω–∞ –∫–æ—Ä–ø—É—Å–µ.
    
    Args:
        data_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ—Ä–ø—É—Å–∞
        model_prefix: –ü—Ä–µ—Ñ–∏–∫—Å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
        vocab_size: –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è (–µ—Å–ª–∏ None, –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
        character_coverage: –ü–æ–∫—Ä—ã—Ç–∏–µ —Å–∏–º–≤–æ–ª–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.9995)
        user_defined_symbols: –û–ø—Ä–µ–¥–µ–ª—è–µ–º—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º —Å–∏–º–≤–æ–ª—ã
        model_type: –¢–∏–ø –º–æ–¥–µ–ª–∏ ('bpe' –∏–ª–∏ 'unigram')
        train_params: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
        
    Returns:
        True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ
    """
    data_file = Path(data_path)
    
    logger.info("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ—Ä–ø—É—Å–∞
    is_valid, issues = validate_corpus(data_file)
    if not is_valid:
        logger.error("‚ùå –û—à–∏–±–∫–∏ –∫–æ—Ä–ø—É—Å–∞:")
        for issue in issues:
            logger.error(f"  - {issue}")
        return False
    
    try:
        with open(data_file, 'r', encoding='utf-8') as f:
            text = f.read()
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
        return False
    
    if not text.strip():
        logger.error("‚ùå –ö–æ—Ä–ø—É—Å –ø—É—Å—Ç –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        return False
    
    #‰º∞ÁÆó —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è
    if vocab_size is None:
        vocab_size = estimate_vocab_size(len(text.encode('utf-8')))
    
    logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ—Ä–ø—É—Å–∞:")
    logger.info(f"   –†–∞–∑–º–µ—Ä: {len(text) / 1_000_000:.2f} MB ({len(text.encode('utf-8')) / 1_000_000:.2f} MB –≤ bytes)")
    logger.info(f"   –°—Ç—Ä–æ–∫: {len(text.splitlines())}")
    logger.info(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã: {len(set(text))}")
    logger.info(f"   –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {vocab_size}")
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    default_params = {
        "input": str(data_file),
        "model_prefix": model_prefix,
        "vocab_size": vocab_size,
        "character_coverage": character_coverage,
        "model_type": model_type,
        "pad_id": 0,
        "bos_id": 1,
        "eos_id": 2,
        "unk_id": 3,
        "hard_vocab_limit": False,
        "user_defined_symbols": user_defined_symbols or ["<newline>", "<tab>", "<url>", "<email>"],
        "normalization_rule_name": "nmt_nfkc",
        "remove_extra_whitespaces": True,
        "split_digits": False,
        "split_by_unicode_script": True,
        "split_by_whitespace": True,
        "treat_whitespace_as_suffix": False,
        "byte_fallback": True,  # –†–µ–∑–µ—Ä–≤–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
        "max_sentencepiece_length": 16,
    }
    
    # –û–±—ä–µ–¥–∏–Ω–∏—Ç—å —Å –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    if train_params:
        default_params.update(train_params)
    
    logger.info(f"‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:")
    logger.info(f"   –¢–∏–ø –º–æ–¥–µ–ª–∏: {default_params['model_type']}")
    logger.info(f"   –ü–æ–∫—Ä—ã—Ç–∏–µ —Å–∏–º–≤–æ–ª–æ–≤: {default_params['character_coverage']}")
    logger.info(f"   –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã: {default_params['user_defined_symbols']}")
    
    try:
        logger.info("üîß –û–±—É—á–µ–Ω–∏–µ SentencePiece –º–æ–¥–µ–ª–∏...")
        spm.SentencePieceTrainer.train(**default_params)
        logger.info("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")
        return False
    
    # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    metadata = {
        "timestamp": datetime.now().isoformat(),
        "vocab_size": vocab_size,
        "character_coverage": character_coverage,
        "model_type": model_type,
        "input_file": str(data_file),
        "model_prefix": model_prefix,
        "corpus_size_bytes": len(text.encode('utf-8')),
        "corpus_size_chars": len(text),
        "corpus_lines": len(text.splitlines()),
        "unique_chars": len(set(text)),
        "user_defined_symbols": default_params.get("user_defined_symbols", [])
    }
    
    try:
        metadata_path = f"{model_prefix}.meta.json"
        with open(metadata_path, "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        logger.info(f"‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metadata_path}")
    except Exception as e:
        logger.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}")
    
    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
    model_file = Path(f"{model_prefix}.model")
    vocab_file = Path(f"{model_prefix}.vocab")
    
    if model_file.exists() and vocab_file.exists():
        logger.info(f"‚úÖ –§–∞–π–ª—ã –º–æ–¥–µ–ª–∏ —Å–æ–∑–¥–∞–Ω—ã:")
        logger.info(f"   {model_file.name} ({model_file.stat().st_size / 1_000:.1f} KB)")
        logger.info(f"   {vocab_file.name} ({vocab_file.stat().st_size / 1_000:.1f} KB)")
        return True
    else:
        logger.error("‚ùå –§–∞–π–ª—ã –º–æ–¥–µ–ª–∏ –Ω–µ —Å–æ–∑–¥–∞–Ω—ã")
        return False

def benchmark_tokenizer(model_prefix: str, test_texts: Optional[List[str]] = None) -> Dict:
    """
    –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –æ–±—É—á–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä.
    
    Args:
        model_prefix: –ü—Ä–µ—Ñ–∏–∫—Å –º–æ–¥–µ–ª–∏
        test_texts: –¢–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
        
    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    """
    model_file = Path(f"{model_prefix}.model")
    
    if not model_file.exists():
        logger.error(f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_file}")
        return {'error': 'Model not found'}
    
    try:
        sp = spm.SentencePieceProcessor()
        sp.load(str(model_file))
        
        if test_texts is None:
            test_texts = [
                "–ü—Ä–∏–≤–µ—Ç, —ç—Ç–æ —Ç–µ—Å—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ SentencePiece!",
                "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ - —ç—Ç–æ –º–æ—â–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç.",
                "–ö–∞–∫ –≤–∞—Å –∑–æ–≤—É—Ç? –ú–µ–Ω—è –∑–æ–≤—É—Ç KINT.",
                "12345 –∏ Some English words –∑–¥–µ—Å—å."
            ]
        
        results = {
            'vocab_size': sp.get_piece_size(),
            'tests': []
        }
        
        logger.info("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞:")
        
        for text in test_texts:
            tokens = sp.encode(text)
            decoded = sp.decode(tokens)
            
            test_result = {
                'text': text,
                'num_tokens': len(tokens),
                'tokens': sp.encode(text, out_type=str)[:10],  # –ü–µ—Ä–≤—ã–µ 10
                'decoded': decoded,
                'compression_ratio': len(text) / len(tokens) if tokens else 0
            }
            results['tests'].append(test_result)
            
            logger.info(f"   –¢–µ–∫—Å—Ç: '{text}'")
            logger.info(f"   –¢–æ–∫–µ–Ω–æ–≤: {len(tokens)}, –°–∂–∞—Ç–∏–µ: {test_result['compression_ratio']:.2f}x")
        
        return results
    
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
        return {'error': str(e)}

if __name__ == "__main__":
    # === –ü–û–õ–ù–´–ô –ö–û–ù–í–ï–ô–ï–† –û–ë–£–ß–ï–ù–ò–Ø ===
    
    logger.info("=" * 60)
    logger.info("–û–ë–£–ß–ï–ù–ò–ï –¢–û–ö–ï–ù–ò–ó–ê–¢–û–†–ê KINT")
    logger.info("=" * 60)
    
    # –≠—Ç–∞–ø 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ—Ä–ø—É—Å–∞
    logger.info("\nüìù –≠—Ç–∞–ø 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ—Ä–ø—É—Å–∞")
    prep_stats = prepare_corpus(
        input_path="data/corpus.txt",
        output_path="corpus_cleaned.txt",
        min_line_length=5,
        aggressive_clean=False
    )
    
    if 'error' in prep_stats:
        logger.error("‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∫–æ—Ä–ø—É—Å–∞")
        exit(1)
    
    # –≠—Ç–∞–ø 2: –û–±—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
    logger.info("\nü§ñ –≠—Ç–∞–ø 2: –û–±—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞")
    success = train(
        data_path="corpus_cleaned.txt",
        model_prefix="tokenizer",
        vocab_size=None,  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
        character_coverage=0.9995,
        user_defined_symbols=["<newline>", "<tab>", "<url>", "<email>", "<code>", "<formula>"],
        model_type="bpe"
    )
    
    if not success:
        logger.error("‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞")
        exit(1)
    
    # –≠—Ç–∞–ø 3: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    logger.info("\n‚úÖ –≠—Ç–∞–ø 3: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞")
    benchmark_results = benchmark_tokenizer("tokenizer")
    
    if 'error' not in benchmark_results:
        logger.info("\n‚ú® –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–ï–®–ù–û!")
        logger.info(f"   –°–ª–æ–≤–∞—Ä—å: {benchmark_results['vocab_size']} —Ç–æ–∫–µ–Ω–æ–≤")
    else:
        logger.error("\n‚ùå –û–®–ò–ë–ö–ê –ü–†–ò –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ò")
